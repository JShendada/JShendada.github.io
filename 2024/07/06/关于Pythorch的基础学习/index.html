
<!DOCTYPE html>
<html lang="ch">
<head>
    <meta charset="utf-8" />
    <title>关于Pythorch的基础学习 | JS&#39;s BLOG</title>
    <meta name="author" content="JShendada" />
    <meta name="description" content="一个正在努力学习AI的同学" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/GM.png" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>



<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>JS&#39;S BLOG</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;JS&#39;S BLOG</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>关于Pythorch的基础学习</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/7/6
        </span>
        
        
    </div>
    
    <div class="content" v-pre>
        <h1 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h1><h2 id="张量的创建和引入第三方库"><a href="#张量的创建和引入第三方库" class="headerlink" title="张量的创建和引入第三方库"></a>张量的创建和引入第三方库</h2><pre><code class="bash">import torch     
x = torch.arange(12)  
</code></pre>
<h2 id="基本函数使用"><a href="#基本函数使用" class="headerlink" title="基本函数使用"></a>基本函数使用</h2><pre><code class="bash">x = torch.arange(12)  #创建行张量
print(x)
print(x.shape)  #张量每个轴的长度
print(x.numel())    #计算张量内所有元素的总和

X = x.reshape(3, 4)     #改变形状不改变大小
print(X)
print(X.shape)
print(X.numel())
print(X.size())
</code></pre>
<p><img src="/images/1.png" alt="第一个的运行结果"><br>**torch.arange(start,end,step)**是生成一个从start开始到end但不包括end结束，<br>步长为step的一个张量。如果step的值使得从start到end无法取到第二个值，结果仅有<br>起始值。<br><strong>size()<strong>和</strong>shape</strong>的运行结果是一致的<br>接下来是其他的创建张量的形式</p>
<pre><code class="bash">x = torch.zeros((2, 3, 4))      #这是三维张量有两个3x4的矩阵
print(x)
x = torch.ones((2, 3, 4))
print(x)
x = torch.rand((2, 3, 4))
print(x)
</code></pre>
<p><img src="/images/2.png" alt="其他张量创建"></p>
<h2 id="运算符的使用"><a href="#运算符的使用" class="headerlink" title="运算符的使用"></a>运算符的使用</h2><pre><code class="bash">x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
print(x + y)    #加法
print(x - y)    #减法
print(x * y)    #乘法
print(x / y)    #除法
print(x ** y)   #幂运算
print(x // y)   #整除
print(x % y)    #取余||模运算
</code></pre>
<p><img src="/images/3.png"></p>
<pre><code class="bash">x = torch.tensor([1.0, 2, 4, 8])
print(torch.exp(x))  #e的几次方
x = torch.arange(12, dtype=torch.float32).reshape(3, 4)
y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
print(torch.cat((x, y), dim=0))  #cat会把张量链接起来 || dim=0为形状的第一个元素即为行
print(torch.cat((x, y), dim=1))  #                 || dim=1为形状的第二个元素即为列
</code></pre>
<p><img src="/images/4.png"></p>
<pre><code class="bash">print(x == y)       #判断对应的元素是否相等
print(x.sum())      #求所有元素的和
</code></pre>
<p><img src="/images/5.png"></p>
<h2 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h2><pre><code class="bash">x = torch.arange(3).reshape(3, 1)
y = torch.arange(2).reshape(1, 2)
print(x + y) 
</code></pre>
<p>由于<strong>形状</strong>本来是不匹配的所以无法进行相加的操作，但是由于广播机制<br>所以对其进行行复制和列复制使其成为一样的形状<br>对于本例子而言即为<br><img src="/images/6.png"></p>
<h2 id="索引和切片"><a href="#索引和切片" class="headerlink" title="索引和切片"></a>索引和切片</h2><pre><code class="bash">import torch

x = torch.arange(12).reshape(3, 4)
print(x[1:3])       #访问2，3行的所有元素
print(x[-1])        #访问最后一行的元素
x[1, 2] = 9         #索引到矩阵的第二行的第三个元素进行改写
print(x)
x[0:2, :] = 12      #将从第一行到第二行的所有元素进行改写
print(x)
x[1, 1:3] = 1       #由上一次类推可得到
print(x)
</code></pre>
<p><img src="/images/7.png"></p>
<h2 id="节省内存"><a href="#节省内存" class="headerlink" title="节省内存"></a>节省内存</h2><pre><code class="bash">y = torch.rand(5)
x = torch.rand(5)

before = id(y)
y = y + x
print(id(y) == before)
</code></pre>
<p>结果显而易见，输出的结果是<strong>false</strong><br>这是因为python首先计算了x + y，为这个结果分配了新的内存，然后Y指向了内存<br>中的这个新位置，这很明显是不可取的：</p>
<h3 id="首先"><a href="#首先" class="headerlink" title="首先"></a>首先</h3><p>我们不想总是不必要的分配内存，因为在机械学习中我们有数百兆的参数，如果都分配<br>内存的话就会浪费大量空间，所以我们希望这些数据能够原地执行这些更新。</p>
<h3 id="其次"><a href="#其次" class="headerlink" title="其次"></a>其次</h3><p>如果我们不原地更新，其他的引用会指向旧的内存，会使我们的某些代码可能会无意的<br>引用旧的参数。</p>
<pre><code class="bash">y = torch.rand(5)
x = torch.rand(5)

z = torch.zeros_like(y)
print(&quot;id(z):&quot;, id(z))
z[:] = x + y
print(&quot;id(z):&quot;, id(z))

before = id(x)
x += y
print(id(x) == before)
</code></pre>
<p>上述方法为<strong>切片表示法</strong>(结果略)<br>切片表示法在Python中是一种高效处理数组和序列的方法。它通过创建原始数据的视图（view）而不是复制来节省内存。这意味着当你使用切片表示法操作数组或序列时，不会创建新的对象，而是创建一个指向相同内存区域的新视图。这种方式在处理大型数据集时尤其有用，因为它可以显著减少内存的使用。<br>第二个表示的方法前提是你的x不会再被使用</p>
<h2 id="转换为其他Python对象"><a href="#转换为其他Python对象" class="headerlink" title="转换为其他Python对象"></a>转换为其他Python对象</h2><pre><code class="bash">y = torch.rand(5)
x = torch.rand(5)

a = x.numpy()
b = torch.tensor(a)
print(type(a), type(b))

a = torch.tensor([3.5])
print(a.item(), float(a), int(a))       #将张量转化为python标量的方法
</code></pre>
<p>上文中的numpy张量和torch张量的区别：<br>numpt不支持GPU加速、自动微分，但numpy主要面向科学计算，提供了丰富的数学和数组操作函数等，只做了解即可。他们两者共享底层内存，就地操作更改一个张量也会更改另一个张量。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>深度学习储存和操作数据的主要接口是张量(n维数组)。它提供了各种功能，包括基本数学运算、广播、索引】切片、内存节省和转换为其他Python对象。</p>
<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p><em>引言</em>:为了能够通过深度学习来解决真实世界的问题，我们经常从预处理原始数据开始，而不是从准备好的张量格式的数据开始。在Python中常用的数据分析中，常用pandas包，接下来即将介绍其用法</p>
<h2 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h2><pre><code class="bash">import os
import pandas as pd

os.makedirs(os.path.join(&#39;..&#39;, &#39;data&#39;), exist_ok=True)      #..在工程文件的根目录下创建
data_file = os.path.join(&#39;..&#39;, &#39;data&#39;, &#39;house_tiny.csv&#39;)
with open(data_file, &#39;w&#39;) as f:
    f.write(&#39;NumRooms,Alley,Price\n&#39;)       # 列名
    f.write(&#39;NA,Pave,127500\n&#39;)             # 数据样本
    f.write(&#39;2,NA,106000\n&#39;)
    f.write(&#39;4,NA,178100\n&#39;)
    f.write(&#39;NA,NA,140000\n&#39;)

data = pd.read_csv(data_file)       #读取csv文件
print(data)
</code></pre>
<p><img src="/images/8.png"></p>
<h2 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h2><p><strong>NAN</strong>代表缺失值。处理缺失的数据的典型方法包括<strong>插值法</strong>和<strong>删除法</strong><br>插值法 ： 用一个替代值弥补缺失值。删除法 ： 直接忽略缺失值。接下来先讲插值法</p>
<pre><code class="bash">inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2] #i索引前两列---o最后一列
inputs = inputs.fillna(inputs.mean())
print(inputs)   
</code></pre>
<p>这是书上的做法，但是经实践这样其实会报错，原因是：<br>Alley值为字符串，并不能直接取平均值，所以会报错，所以正确做法是：</p>
<pre><code class="bash">inputs[&#39;NumRooms&#39;] = inputs[&#39;NumRooms&#39;].fillna(inputs[&#39;NumRooms&#39;].mean())
inputs = inputs.fillna(inputs.mean(numeric_only=True))
</code></pre>
<p>第一种方法只针对单一类别，第二种方法针对全部数值类型的列<br><img src="/images/9.png"></p>
<pre><code class="bash">inputs = pd.get_dummies(inputs, dummy_na=True)
print(inputs)
</code></pre>
<p><img src="/images/10.png"><br>这样对Alley值的类别自动分为Pave和NaN，缺失的pave &#x3D; 0， nan &#x3D; 1.</p>
<h2 id="转换为张量格式"><a href="#转换为张量格式" class="headerlink" title="转换为张量格式"></a>转换为张量格式</h2><pre><code class="bash">outputs = outputs.astype(&#39;float32&#39;)
x, y = torch.tensor(inputs.values), torch.tensor(outputs.values)
print(x, y)
</code></pre>
<p>这段代码目前一直报错：TypeError: can’t convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.所以暂时存疑日后解决。</p>
<h1 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h1><h2 id="标量"><a href="#标量" class="headerlink" title="标量"></a>标量</h2><pre><code class="bash">import torch

x = torch.tensor(3.0)
y = torch.tensor(2.0)
print(x + y, x * y)     # 略
</code></pre>
<p>不做过多介绍</p>
<h2 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h2><pre><code class="bash">x = torch.arange(4)
print(x, x[3])  # 也可以用切片截取
print(len(x), x.shape)
</code></pre>
<p>同上</p>
<h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h2><pre><code class="bash">import torch

x = torch.arange(20).reshape(4, 5)
print(x)
print(x.T)  # X矩阵的转置
</code></pre>
<h2 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h2><p>张量是描述具有任意数量轴的n维数组的通用方法。</p>
<h2 id="张量算法的基本性质"><a href="#张量算法的基本性质" class="headerlink" title="张量算法的基本性质"></a>张量算法的基本性质</h2><pre><code class="bash">import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()
print(A, A + B, A * B)  # 这里的乘法是对应位置相乘(哈达玛积)，如果形状不同则利用前面学到的广播机制
a = 2
print(a + A, (a * A).shape)  # 标量不改变张量的形状，每个元素和标量相加乘
</code></pre>
<p><img src="/images/11.png"></p>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><pre><code class="bash">A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
print(A.size(), A.sum())
</code></pre>
<p>我们可以表示任意张量的元素和,但是我们也可以指定某个轴来进行降维求和等操作</p>
<pre><code class="bash">A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
print(A.size(), A.sum())
print(A.sum(dim=0))     # dim = 0为沿轴0降维，= 1则为对列求和
print(A.sum(dim=1))
print(A.sum(dim=[0, 1]))   # 结果等于A.sum()
print(A.mean(dim=0))    # 沿轴求平均值
</code></pre>
<p>对于文章中指定轴列使用axis但本文的数据并不符合，所以使用dim更好<br><img src="/images/12.png"></p>
<h3 id="非降维求和"><a href="#非降维求和" class="headerlink" title="非降维求和"></a>非降维求和</h3><pre><code class="bash">A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
sum_A = A.sum(dim=1, keepdim=True)
print(A / sum_A)    # 通过广播机制，对每行求和后仍保持一个轴
print(A.cumsum(dim=0))  # 不会沿任何轴降低降低输入张量的维度
</code></pre>
<p><img src="/images/13.png"><br>竖着观察,0,0 + 4, 0 + 4 + 8以此类推</p>
<h2 id="点积"><a href="#点积" class="headerlink" title="点积"></a>点积</h2><pre><code class="bash">y = torch.ones(4, dtype=torch.float32)
x = torch.arange(4, dtype=torch.float32)
print(torch.dot(x, y))
</code></pre>
<p>结果是: tensor(6.)<br>点积在很多场景都有用。例如，给定一组由向量$$x \in R^d$$</p>

    </div>
    
    
    
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2024 JS&#39;s BLOG
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;JShendada
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
    




    
</body>
</html>
