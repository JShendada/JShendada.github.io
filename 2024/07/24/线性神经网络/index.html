
<!DOCTYPE html>
<html lang="ch">
<head>
    <meta charset="utf-8" />
    <title>线性神经网络 | JS&#39;s BLOG</title>
    <meta name="author" content="JShendada" />
    <meta name="description" content="一个正在努力学习AI的同学" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/GM.png" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>


<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css" />
<script src="/js/lib/math.js"></script>


<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>JS&#39;S BLOG</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;JS&#39;S BLOG</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>线性神经网络</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/7/24
        </span>
        
        
    </div>
    
    <div class="content" v-pre>
        <h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p><em>回归</em>:是能为一个或多个自变量和因变量之间的关系建模的唯一方法。再自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。<br>在机器学习当中，大多数任务通常与预测有关。当想要预测一个数值时，就会用到回归。例如：预测价格、住院时长等等</p>
<h2 id="线性回归的基本元素"><a href="#线性回归的基本元素" class="headerlink" title="线性回归的基本元素"></a>线性回归的基本元素</h2><p>线性回归基于几个简单的<em>假设</em>:首先，假设自变量$x$和因变量$y$之间的关系时呈线性的，即y可以表示为x中元素的加权和，这里通常允许观测值包含一些噪声(即数据中存在误差或干扰)；其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。<br>一些真实的数据集称为<em>训练集</em>，每行数据称为<em>数据样本</em>或<em>数据点</em>或<em>数据实例</em>。试图预测的目标称为<em>标签</em>或<em>目标</em>。预测所依据的自变量称为<em>特征</em>或<em>协变量</em></p>
<h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><p>$price &#x3D; \omega_{area} \cdot area + \omega_{age} \cdot age + b$<br>上式中的$\omega$均为<em>权重</em>的意思，权重决定了每个特征对我们预测值的影响。b称为<em>偏置</em>、<em>偏移量</em>或<em>截距</em>。<br><em>偏置</em>：指的是当所有特征都取值为0时，预测值应该为多少。即使现实中不会有任何房屋的面积是0或是房龄正好是0年，我们仍然需要偏置项。如果没有偏置项，我们的模型的表达能力将受到限制。严格来说，上式是输入特征的一个<em>仿射变换</em>。<em>仿射变换</em>的特点是通过加权和对特征进行线性变换，并通过偏置项进行平移。<br>所以我们可知我们的目标是，寻找模型的权重$\omega$和偏置b，使得根据模型的预测大体符合数据中的真实价格。输出的预测值由输入特征通过线性模型的仿射变换来确定，仿射变换由所选权重和偏置确定。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>在开始考虑如何用模型拟合数据之前，我们需要确定拟合程度的度量。<br><em>损失函数</em>能够量化目标的实际值与预测值之间的差距。通常我们需要选择<em>非负数</em>作为损失，且数值越小表示损失越小，完美预测损失则为0.回归中最常用的损失函数是<em>平方误差函数</em>.<br>$l(\omega,b) &#x3D; \frac{1}{2}(y_1 - y)^2$<br>上述式子中$y_1$为样本的预测值,$y$为真实标签，式子中的$\frac{1}{2}$并不会带来本质的影响，但这样在形式上稍微简单一些(因为当我们对损失函数求导后常数系数为1)。由于数据集并不受我们控制，因此经验误差只是关于模型参数的函数。<br>由于平方误差函数中的二次方项，估计值$y_1$和观测值$y$之间较大的差距将会导致更大的损失。为了度量模型在整个数据集上的预测质量，我们需要计算在训练集n个样本的损失均值(等价于求和):<br>$L(\omega,b) &#x3D; \frac{1}{n}\sum_{i&#x3D;1}^{n}\frac{1}{2}(\omega^T\hat{x}_i+b-\hat{y}_i)^2$</p>
<h3 id="解析解"><a href="#解析解" class="headerlink" title="解析解"></a>解析解</h3><p>线性回归是一个简单的优化问题，我们可以用一个公式简单的表示其解，这类解叫 <em>解析解</em> 。合并的方法是在包含所有参数的矩阵附加一列。我们的预测问题是最小化 $||y - X\omega||^2$。在这损失平面上只有一个临界点，这个临界点对应于整个区域的损失极小值点。将损失关于 $\omega$ 的导数设为0，得到解析解:<br>$\omega^* &#x3D; (X^TX)^{-1}X^Ty$<br>像线性回归这样的简单问题存在解析解，但并不是所有问题都存在解析解，所以解析解并不能广泛应用于深度学习中。</p>
<h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>即使无法求出解析解，也可以有效的优化模型。而这里的<em>梯度下降</em>的方法几乎可以优化所有深度学习模型。它通过不断在损失函数递减的方向上更新参数来降低误差。<br>梯度下降的最简单的用法是计算损失函数(数据集中所有样本的损失均值)关于模型参数的导数。但由于数据集可能过于庞大，遍历全部可能会非常慢。因此我们通常会在更新的时候随机抽一小批样本，这种变体叫小批量随机梯度下降。<br>在每次迭代中，我们先随机抽取一个小批量B，它是由固定数量的训练样本组成的；然后计算小批量的损失值关于模型参数的导数；最后，将梯度乘以一个预先确定的正数 $\eta$，并从当前参数的值中减掉。<br>$(\omega,b)\leftarrow(\omega,b)-\frac{\eta}{\lvert B \rvert}\sum_{i \in B}\theta_{(\omega,b)} l^{(i)}(\omega,b)$<br>其中$\lvert B \rvert$为每个小批量中的样本数，也称批量大小。$\eta$ 表示学习率。批量大小和学习率的值通常是预先手动指定，而不是通过模型训练得到的。这些可以调整但不在训练过程中更新的称为<em>超参数</em>。<em>调参</em>是选择超参数的过程。超参数通常是我们根据训练迭代结果来调整的，而训练迭代结果是在独立的<em>验证数据集</em>上评估得到的。<br>在训练了若干次后，我们记录下模型参数的估计值，表示为 $\hat{\omega},\hat{b}$。但是即使得到的函数是线性无噪声的也不能说明达到最小值了，我们只能说明这种情况是将损失向最小值收敛而不是精准的达到最小值。<br>在这个线性回归的过程中只有一个最小值，而在深度神经网络中，损失平面上通常包括多个最小值。我们通常不去寻找这组参数，而事实上，我们更难的是找到一组数据使得其能够在我们从未见过的数据上实现较小的损失，这一挑战称为泛化。</p>
<h3 id="用模型进行预测"><a href="#用模型进行预测" class="headerlink" title="用模型进行预测"></a>用模型进行预测</h3><p>在给定特征的情况下估计目标的过程通常称为预测或推断。推断更多地表示基于数据集估计参数。当深度学习从业者与统计学家交流时，术语的误用经常导致一些误解。</p>
<h2 id="向量化加速"><a href="#向量化加速" class="headerlink" title="向量化加速"></a>向量化加速</h2><p>在 PyTorch 中，向量化加速是指利用向量和矩阵操作来替代显式的循环，从而提高计算效率。这种方法利用了现代 CPU 和 GPU 的并行计算能力，可以显著加速大规模数据的处理。</p>
<pre><code class="bash">import torch

a = torch.tensor([1.0, 2.0, 3.0])
b = torch.tensor([4.0, 5.0, 6.0])

dot_product = 0.0
for i in range(len(a)):
    dot_product += a[i] * b[i]

print(dot_product)
</code></pre>
<pre><code class="bash">import torch

a = torch.tensor([1.0, 2.0, 3.0])
b = torch.tensor([4.0, 5.0, 6.0])

dot_product = torch.dot(a, b)

print(dot_product)
</code></pre>
<p>在第二个示例中，torch.dot(a, b) 是一个向量化操作，它利用了底层硬件的并行计算能力，比显式循环更高效。  <em>个人认为:就是多利用库内的向量操作来实现加速</em></p>
<h2 id="正态分布与平方损失"><a href="#正态分布与平方损失" class="headerlink" title="正态分布与平方损失"></a>正态分布与平方损失</h2><p>正态分布的知识自行了解<br>总而言之，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。</p>
<h2 id="从线性回归到深度网络"><a href="#从线性回归到深度网络" class="headerlink" title="从线性回归到深度网络"></a>从线性回归到深度网络</h2><h3 id="神经网络图"><a href="#神经网络图" class="headerlink" title="神经网络图"></a>神经网络图</h3><p>假设输入为 $X_1,x_2….$ ，每个输入层值对应一个输出为 $o_1$ 的话，其层数为1。所以线性回归模型可以视为单个人工神经元组成的神经网络，或称为单层神经网络。<br>对于线性回归，每个输入都有每个输出链接，我们将这种称为全连接层或稠密层。</p>
<h3 id="生物学"><a href="#生物学" class="headerlink" title="生物学"></a>生物学</h3><p>略</p>
<h1 id="线性回归的从零开始实现"><a href="#线性回归的从零开始实现" class="headerlink" title="线性回归的从零开始实现"></a>线性回归的从零开始实现</h1><p>代码实现!!</p>
<h2 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h2><p>简单起见，我们将根据带有噪声的线性模型构造一个数据集。我们的任务是根据这个有限样本的数据集来恢复这个模型的参数。我们将使用低维数据，这样可以很容易地将其可视化，并且每个样本包含从标准正态分布中抽样的两个特征。我们的合成数据集是一个矩阵$X \in R^{1000×2}$<br>我们使用线性模型参数 $\omega &#x3D; [2,-3.4]^T、b &#x3D; 4.2 和噪声项\epsilon$来生成数据集及其标签<br>$y &#x3D; X\omega + b + \epsilon$<br>$\epsilon$ 我们可以视为模型预测和标签的潜在观测误差。这里我们认为标准假设成立，即$\epsilon$ 服从标准正态分布。为了简化问题我们将标准差设为0.01。</p>
<pre><code class="bash">import random
import torch
from d2l import torch as d2l


def synthetic_data(w, b, num_examples):
    &quot;&quot;&quot; 生成y = wx + b + 噪声&quot;&quot;&quot;
    x = torch.normal(0, 1, (num_examples, len(w)))  # normal 为高斯分布
    y = torch.matmul(x, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return x, y.reshape((-1, 1))  # 返回列向量


true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
d2l.set_figsize()
d2l.plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1)
d2l.plt.show()
</code></pre>
<p><img src="/images/15.png"><br>通过观察上图,我们可以发现二者的线性关系。</p>
<h2 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h2><p>由于在训练过程中，我们需要对整个数据集遍历并且分批次抽少量样来更新模型，所以必不可少的是定义一个函数来打乱数据集并以小批量获取数据。</p>
<pre><code class="bash">def data_iter(batch_size, features, labels):
    # 获取特征数据的样本数量
    num_examples = len(features)
    
    # 创建一个包含从 0 到 num_examples-1 的索引列表
    indices = list(range(num_examples))
    
    # 随机打乱索引列表
    random.shuffle(indices)
    
    # 使用 for 循环遍历索引列表，每次步长为 batch_size
    for i in range(0, num_examples, batch_size):
        # 计算当前批次的索引范围，并将这些索引转换为 PyTorch 张量
        batch_indices = torch.tensor(indices[i:min(i+batch_size, num_examples)])
        
        # 返回当前批次的索引、特征数据和标签数据
        yield batch_indices, features[batch_indices], labels[batch_indices]


batch_size = 10
for x, y in data_iter(batch_size, features, labels):
    print(x, &#39;\n&#39;, y)
    break
</code></pre>
<p><img src="/images/16.png"><br>上述代码中，通过接收批量大小，特征矩阵和标签向量作为输入，生成大小为batch_size的小批量。并且RT，特征v矩阵为二维，标签向量为一维。<br>上述代码的实际效率很低，并且面对庞大的数据集时，我们需要意味着需要大量的内存访问，所以这个方法其实很低效。深度学习框架中实现的内置迭代器效率要高得多,它何以处理存储在文件中的数据和数据流提供的数据。</p>
<h2 id="初始化参数模型"><a href="#初始化参数模型" class="headerlink" title="初始化参数模型"></a>初始化参数模型</h2><pre><code class="bash">w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
</code></pre>
<p>先通过从均值为0,标准差为0.01的正态分布中抽样随机数来初始化权重.并将偏置初始化为0<br>初始化之后我们的目的是更新参数直到能够拟合我们的数据。所以每次更新都要计算损失函数关于我们参数的梯度。</p>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><pre><code class="bash">def linreg(x, w, b):
    return torch.matmul(x, w) + b
</code></pre>
<p>上述为线性回归模型</p>
<h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><pre><code class="bash">def squared_loss(y_hat, y):
    return (y_hat - y.reshape(y_hat.shape))**2 / 2
</code></pre>
<p>上述为均方损失，并且对真实值y可能是行向量和列向量，所以为了统一起见我们把它reshape成和预测值$y_hat$一样。</p>
<h2 id="定义优化算法"><a href="#定义优化算法" class="headerlink" title="定义优化算法"></a>定义优化算法</h2><pre><code class="bash">def sgd(params, lr, batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
</code></pre>
<p>上述代码实现了对小批量随机梯度下降的更新。<br>使用 torch.no_grad() 上下文管理器，表示在这个上下文中进行的操作不会被记录到计算图中。这是为了避免在更新参数时计算梯度，从而节省内存和计算资源。</p>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><pre><code class="bash">lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss
for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)
        l.sum().backward()
        sgd([w, b], lr, batch_size)
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f&#39;epoch &#123;epoch + 1&#125;, loss &#123;float(train_l.mean()):f&#125;&#39;)
</code></pre>
<p>上述函数中的轮数和学习率都是超参数，我们需要根据所得的数据去更改这两个超参数来获得接近的值。<br>同时我们也不必担心得不到完美的值，因为即使在复杂的情况下，也存在许多种参数的组合能够实现较高的预测。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>在这一章我们简单的学习了神经网络是如何实现和优化的，只运用了张量和自动微分，并没有使用定义层和复杂的优化器，这些日后会学到。</p>
<h1 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h1><p>本节将利用现代的深度学习库来实现诸如数据迭代器、损失函数、优化器和神经网络层(简单易操作)</p>
<pre><code class="bash">import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l
from torch import nn

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)


def load_array(data_arrays, batch_size, is_train=True):
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)


batch_size = 10
data_iter = load_array((features, labels), batch_size)

net = nn.Sequential(nn.Linear(2, 1))

net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

loss = nn.MSELoss()
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X), y)
        trainer.zero_grad()
        l.backward()    #不需要求和因为pytorch已经帮你求完了
        trainer.step()
    l = loss(net(features), labels)
    print(f&#39;epoch &#123;epoch + 1&#125;, loss &#123;l:f&#125;&#39;)
</code></pre>
<p>使用 data.TensorDataset 创建一个数据集对象 dataset。data_arrays 是一个包含特征和标签的元组，*data_arrays 将元组解包为位置参数传递给 TensorDataset。<br>使用 data.DataLoader 创建一个数据加载器对象 DataLoader，并返回它。<br>dataset：要加载的数据集。<br>batch_size：每个批次的大小。<br>shuffle：是否在每个 epoch 开始时打乱数据。如果 is_train 为 True，则打乱数据<br>清零优化器的梯度 trainer.zero_grad()。<br>反向传播计算梯度 l.backward()。<br>更新模型参数 trainer.step()<br>上述代码可以和上一张进行对比即可了解这段代码运用的第三方库的作用。</p>
<h1 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h1><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>假设输入一个2×2的灰度图像，我们可以用一个标量来表示每个像素，每个图像对应着四个特征x1,x2,x3,x4。此外，假设每个图像属于类别“猫”“鸡”和“狗”中的一个。<br>对于标签我们有两个明显的选择：最直接的办法是选择 $y \in {1,2,3}$,其中分别代表{狗、猫、鸡}。如果类别之间有一些自然顺序，比如我们试图预测{婴儿，儿童，青少年，青年人，中年人，老年人}，那么这个问题将转变为回归问题，并且保留这种格式是有意义的。<br>但是一般的分类问题的类别并不与类别之间的自然顺序有关。幸运的是，统计学家很早以前就发明了一种表示分类数据的简单方法：<em>独热编码</em>。独热编码是一个向量，它的分量和类别一样多。类别对应的分量设为1，其他的分量设为。例如，(1,0,0)对应猫，(0,1,0)对应鸡,(0,0,1)对应狗。</p>
<h2 id="网络构架"><a href="#网络构架" class="headerlink" title="网络构架"></a>网络构架</h2><p>为了评估所有可能类别的条件概率，我们需要一个有多个输出的模型，每个模型对应一个类别。为了解决线性模型的分类问题，我们徐奥和输出一样多的放射函数。每个输出对应于它自己的放射函数。在上述例子中，由于我们有4个特征（即输入）和三个可能的类别，五哦一我们需要12个标量来表示权重，3个标量来表示偏置。<br><img src="/images/17.png"><br>如图所示即为这个计算过程，由于每个输出取决于每个输入，所以softmax回归也是全连接层。<br>为了更简洁的表达。我们可以通过向量形式表达o &#x3D; Wx + b，权重我们放到了一个3×4的矩阵当中。</p>
<h2 id="全连接层的参数开销"><a href="#全连接层的参数开销" class="headerlink" title="全连接层的参数开销"></a>全连接层的参数开销</h2><p>在深度学习中处处皆有全连接层，然而这其中会用很多参数，具体的说，对于任何有d个输入和q个输出的全连接层，参数开销为O(dq)，这个数字在实践中可能会非常高，但幸运的是，我们可以将其成本减少到$O(\frac{dq}{n})$，其中n是超参数，以平衡参数节约和模型有效性。</p>
<h2 id="softmax-运算"><a href="#softmax-运算" class="headerlink" title="softmax 运算"></a>softmax 运算</h2><p>对于未规范化的预测o我们能否视为我们感兴趣的输出呢？答案是否定的。因为线性层的输出直接视为概率会存在以下问题：1.没限制输出数字的总和为1。2.输入不同会导致输出为负值这并不符合概率论。所以要视输出为概率，我们需要解决上述两个问题。<br>此外，我们还需要一个训练的目标函数，来激励模型精准地估计概率。例如，在分类器输出为0.5的样本中，我们希望这些样本刚好有一半实际上属于预测的类别。这个属性叫做<em>校准</em>。<br>$\hat{y} &#x3D; softmax(o)  \hat{y_j}&#x3D;\farc{exp(o_j)}{\sum{k}exp(o_k)} $<br>上述式子即是softmax函数。对于所有j总有其y_j在0到1，因此，$\hat{y}$可以视为一个正确的概率分布。同时我们可以用 argmax$\hat{y_j}$ &#x3D; argmax$o_j$<br>尽管softmax是非线性的，但是softman回归的输出仍是由其输入的特征的仿射变换所决定的。因此，softmax回归是一个线性模型。</p>
<h2 id="小批量样本的矢量化"><a href="#小批量样本的矢量化" class="headerlink" title="小批量样本的矢量化"></a>小批量样本的矢量化</h2><p>为了提高计算效率，我们会对小批量样本的数据执行矢量计算。假设读取了样本X，其中特征维度为d，批量为n，输出有q个类别。那么小批量特征$X \in R^{n×q}$,权重$W \in R^{d×q}$,偏置$b \in R^{1×q}$<br>Softmax: O &#x3D; XW + b<br>对于o中的每一行，我们可以先对所有项先进行幂运算然后,然后通过求和对其进行标准化，结果为n × q的矩阵(广播机制)</p>
<h2 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h2><p>这里我们用到的是交叉熵损失，即对于标签y和预测模型$\hat{y}$<br>$l(y,\hat{y}) &#x3D; -\sum{j&#x3D;1}{q}y_jlog\hat{y_j}$<br>这里因为y是长为q的独热编码，所以除了真实的一项为1，其余均为0，又因为$\hat{y_j}$是预测的概率，所以其对数不大于0所以加符号。这里之所以应用交叉熵是因为交叉熵后模型的差值会很大，能更明显的体现出不同模型之间的差异，这是我们对分类问题所期望看到的。</p>
<h2 id="模型的预测和评估"><a href="#模型的预测和评估" class="headerlink" title="模型的预测和评估"></a>模型的预测和评估</h2><p>在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。通常我们使用预测概率最高的类别作为输出类别。如果预测和实际一样则成功。我们用<em>精度</em>来评判一个模型的性能，其等于正确预测数和预测总是之间的比率。</p>
<h1 id="图像分类数据集"><a href="#图像分类数据集" class="headerlink" title="图像分类数据集"></a>图像分类数据集</h1><pre><code class="bash">import torch
import torchvision
from torch.utils import data
from torchvision import transforms
from d2l import torch as d2l
import matplotlib.pyplot as plt


def get_dataloader_workers():   # 便于操作后续更改直接在这里更改。
    return 4


def load_data_fashion_minst(batch_size, resize=None):
    trans = [transforms.ToTensor()]     # 创建一个包含将图片转换为向量模式的列表
    if resize:  
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)   # 串联多个图片变换的操作
    mnist_train = torchvision.datasets.FashionMNIST(root=&quot;../data&quot;, train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(root=&quot;../data&quot;, train=False, transform=trans, download=True)
    return (data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()),
            data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=get_dataloader_workers()))


train_iter, test_iter = load_data_fashion_minst(32, resize=64)
</code></pre>
<p>上述代码即是对图像进行下载和读取操作，并且通过dataloader中的shuffle实现对训练集的小批量读取。</p>
<h2 id="开始从零实现softmax"><a href="#开始从零实现softmax" class="headerlink" title="开始从零实现softmax"></a>开始从零实现softmax</h2><pre><code class="bash">import torch
import torchvision
from torch.utils import data
from torchvision import transforms
from d2l import torch as d2l
import matplotlib.pyplot as plt
from IPython import display


def get_dataloader_workers():
    return 4


def load_data_fashion_minst(batch_size, resize=None):
    trans = [transforms.ToTensor()]
    if resize:
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(root=&quot;../data&quot;, train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(root=&quot;../data&quot;, train=False, transform=trans, download=True)
    return (data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()),
            data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=get_dataloader_workers()))


batch_size = 256
train_iter, test_iter = load_data_fashion_minst(batch_size)
num_inputs = 784
num_outputs = 10

W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)


def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition    # 广播机制


def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)


def cross_entropy(y_hat, y):
    return -torch.log(y_hat[range(len(y_hat)), y])


&quot;&quot;&quot;
 这里必须用range，相当于遍历每行，对该行找对应的那个类的预测概率如果用y_pred[:,y_true] 相当于返回指定的列，而不是概率
 y_hat = torch.tensor([[0.1, 0.3, 0.6], 
                      [0.2, 0.5, 0.3], 
                      [0.7, 0.2, 0.1]])
 y = torch.tensor[(2, 1, 0)]
 第二段代码展示的则是:
 tensor([[0.6, 0.3, 0.1],
        [0.3, 0.5, 0.2],
        [0.1, 0.2, 0.7]]),
 而第一段则是
 tensor([0.6, 0.5, 0.7])
&quot;&quot;&quot;


def accuracy(y_hat, y):
    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1:
        y_hat = y_hat.argmax(axis=1)    # 之所以取最大值，是因为softmax函数的输出是一个概率分布，取最大值即为预测的类别
        cmp = y_hat.type(y.dtype) == y
        return float(cmp.type(y.dtype).sum())


def evaluate_accuracy(net, data_iter):
    if isinstance(net, torch.nn.Module):
        net.eval()  # 评估模式，关闭dropout
    metric = d2l.Accumulator(2)     # 将metric设置为一个累加迭代器，用于存储正确预测的数量和总数量
    for X, y in data_iter:
        metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]


def train_epoch_ch3(net, train_iter, loss, updater):
    if isinstance(net, torch.nn.Module):
        net.train()
    metric = d2l.Accumulator(3)
    for X, y in train_iter:
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()
            l.meac().backward()
            updater.step()
        else:
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    return metric[0] / metric[2], metric[1] / metric[2]


def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
    animator = d2l.Animator(xlabel=&quot;epoch&quot;, xlim=[1, num_epochs], legend=[&quot;train loss&quot;, &quot;train acc&quot;, &quot;test acc&quot;])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss &lt; 0.5, train_loss
    assert train_acc &lt;= 1 and train_acc &gt; 0.7, train_acc
    assert test_acc &lt;= 1 and test_acc &gt; 0.7, test_acc


lr = 0.1


def updater(batch_size):
    return d2l.sgd([W, b], lr, batch_size)


num_epochs = 10
train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
plt.show()
</code></pre>
<p><img src="/images/18.png"></p>
<h2 id="简单实现softmax"><a href="#简单实现softmax" class="headerlink" title="简单实现softmax"></a>简单实现softmax</h2><pre><code class="bash">net = nn.Sequential(nn.Flatten(), nn.Linear(num_inputs, num_outputs))


def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)


net.apply(init_weights)
loss = nn.CrossEntropyLoss(reduction=&#39;none&#39;)
trainer = torch.optim.SGD(net.parameters(), lr=0.1)
</code></pre>
<p>其中的简易实现就是把相应的操作进行替换，然后最后使用3.6中的训练函数继续训练即可。</p>

    </div>
    
    
    
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2024 JS&#39;s BLOG
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;JShendada
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
    




    
</body>
</html>
